{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://huggingface.co/speakleash/Bielik-7B-Instruct-v0.1/raw/main/speakleash_cyfronet.png\">\n",
        "</p>"
      ],
      "metadata": {
        "id": "1xkOXgayM1Au"
      },
      "id": "1xkOXgayM1Au"
    },
    {
      "cell_type": "markdown",
      "id": "b315ecd49bdab447",
      "metadata": {
        "id": "b315ecd49bdab447"
      },
      "source": [
        "# Bielik + Ollama – czyli proste uruchomienie LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eae5e1055075fd",
      "metadata": {
        "id": "7eae5e1055075fd"
      },
      "source": [
        "LLM można używać bezpośrednio korzystając z biblioteki **transformers** od HuggigFace, lecz na dłuższą metę może okazać się to uciążliwe – np. trzymanie modelu w obiektach i zarządzanie nimi (często powiązane z obsługą GPU).\n",
        "\n",
        "Z pomocą przychodzą różne narzędzia takie jak: **Ollama**, **vLLM**, **Text Generation Interface (TGI)** czy **llama.cpp** – w tym tutorialu, ze względu na prostotę, zostanie omówione pierwsze rozwiązanie."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a294d7a06f0f8712",
      "metadata": {
        "id": "a294d7a06f0f8712"
      },
      "source": [
        "## Ollama\n",
        "\n",
        "Ollama to narzędzie do uruchamiania i zarządzania dużymi modelami językowymi na lokalnym komputerze. Zapewnia prosty interfejs do korzystania z gotowych modeli, jak również możliwość ich personalizacji i dostosowywania za pomocą specjalnych plików konfiguracyjnych. Ollama jest dostępna na różnych platformach, w tym macOS, Windows oraz Linux.\n",
        "\n",
        "Przydatne linki:\n",
        "  - Oficjalna strona projektu: https://ollama.com\n",
        "  - Github: https://github.com/ollama/ollama\n",
        "  - Dokumentacja API: https://github.com/ollama/ollama/blob/main/docs/api.md"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31033784729db073",
      "metadata": {
        "id": "31033784729db073"
      },
      "source": [
        "### Ollama instalacja\n",
        "\n",
        "Narzędzie jest wspierane na platformach macOS, Windows oraz Linux. Należy zainstalować je jako klasyczną aplikację. Na platformach Windows i macOS będzie to możliwe poprzez klasyczne pobranie i zainstalowanie pakietu, natomiast na Linuxie będzie trzeba uruchomić specjalną komendę *[stan na sierpień 2024]*.\n",
        "\n",
        "Szczegółowe informacje znajdują się na stronie projektu.\n",
        "\n",
        "Aplikacja działa w oparciu o Command Line Interface (CLI) – wiersz poleceń. W celu weryfikacji poprawnej instalacji zaleca się  uruchomienie komendy: `ollama`. Przykładowy wynik jej działania został przedstawiony poniżej (a dokładnie instrukcja zawierająca komendy do obsługi tego narzędzia). Jeżeli pojawią się błędy, może oznaczać to, że narzędzie nie zostało zainstalowane poprawnie.\n",
        "\n",
        "```shell\n",
        "(base) sample_user@MacBook-Pro SPEAKLEASH % ollama\n",
        "Large language model runner\n",
        "\n",
        "Usage:\n",
        "  ollama [flags]\n",
        "  ollama [command]\n",
        "\n",
        "Available Commands:\n",
        "  serve       Start ollama\n",
        "  create      Create a model from a Modelfile\n",
        "  show        Show information for a model\n",
        "  run         Run a model\n",
        "  pull        Pull a model from a registry\n",
        "  push        Push a model to a registry\n",
        "  list        List models\n",
        "  ps          List running models\n",
        "  cp          Copy a model\n",
        "  rm          Remove a model\n",
        "  help        Help about any command\n",
        "\n",
        "Flags:\n",
        "  -h, --help      help for ollama\n",
        "  -v, --version   Show version information\n",
        "\n",
        "Use \"ollama [command] --help\" for more information about a command.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fe1a72a6ffada73",
      "metadata": {
        "id": "9fe1a72a6ffada73"
      },
      "source": [
        "### Ollama uruchomienie poprzez CLI\n",
        "W zaprezentowanym przykładzie wykorzystamy model Bielik-11B-v2.2-Instruct-GGUF formacie kwantyzacji Q4_K_M:<br>\n",
        "```bielik-11b-v2.2-instruct:Q4_K_M``` <br>\n",
        "\n",
        "Więcej informacji na temat modelu można znaleźć pod linkiem:<br>\n",
        "https://huggingface.co/speakleash/Bielik-11B-v2.2-Instruct-GGUF<br>\n",
        "\n",
        "Natomiast informacje dotyczące pozostałych formatów kwantyzacji, które są dostępne w serwisie Ollama, można sprawdzić na stronie:<br>\n",
        "https://ollama.com/SpeakLeash/bielik-11b-v2.2-instruct:Q4_K_M"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aby skorzystać z modelu należy go uprzednio pobrać za pomocą komendy:\n",
        "```shell\n",
        "ollama pull SpeakLeash/bielik-11b-v2.2-instruct:Q4_K_M\n",
        "```\n",
        "\n",
        "Następnie model można uruchomić stosując polecenie:\n",
        "\n",
        "```shell\n",
        "ollama run SpeakLeash/bielik-11b-v2.2-instruct:Q4_K_M\n",
        "```\n",
        "\n",
        "Docelowo można pominąć pierwszy krok, czyli pobieranie, ponieważ uruchomienie modelu za pomocą komedny ```run``` automatycznie sprawdzi, czy jest on dostępny w lokalnej bazie. Jeżeli tak nie jest, to rozpocznie się jego pobieranie.<br>\n",
        "Po poprawnym załadowaniu będzie można zacząć korzystać z modelu w konsoli.\n",
        "\n",
        "```shell\n",
        "(base) marcinwatroba@DataScienceServer:~$ ollama run SpeakLeash/bielik-11b-v2.2-instruct:Q4_K_M\n",
        ">>> Ile to jest 2+4?\n",
        "<s>  2 plus 4 równa się 6.\n",
        "\n",
        ">>> Kto napisał Pana Tadeusza?\n",
        "<s>  Adam Mickiewicz napisał \"Pana Tadeusza\".\n",
        "\n",
        ">>>\n",
        "```"
      ],
      "metadata": {
        "id": "_1pIxP_DWpQu"
      },
      "id": "_1pIxP_DWpQu"
    },
    {
      "cell_type": "markdown",
      "id": "cc59aec5b7383d70",
      "metadata": {
        "id": "cc59aec5b7383d70"
      },
      "source": [
        "### Ollama – korzystanie z API (curl)\n",
        "\n",
        "Ollama, poza korzystaniem z CLI, udostępnia również możliwość komunikacji poprzez Rest API.\n",
        "Pozwala to na korzystanie z modelu w praktycznie każdej aplikacji, co daje całkiem spore możliwości.\n",
        "\n",
        "Domyślnie aplikacja działa na porcie 11434. Oznacza to, że jej domyślny host to http://localhost:11434 i z niego będziemy korzystać w poniższych przykładach.\n",
        "\n",
        "Najprostszym przykładem będzie po prostu uruchomienie klasycznego zapytania dla zapytania składającego się z listy wiadomości (szczegóły dotyczące API znajdują się pod adresem https://github.com/ollama/ollama/blob/main/docs/api.md).\n",
        "\n",
        "Poniżej znajduje się przykład zapytania w popularnej bibliotece `curl` -- można jej używać w formie CLI, bez większych umiejętności programowania."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83761122309b676a",
      "metadata": {
        "id": "83761122309b676a"
      },
      "source": [
        "```shell\n",
        "curl http://localhost:11434/api/chat -d '{\n",
        "  \"model\": \"SpeakLeash/bielik-11b-v2.2-instruct:Q4_K_MF\",\n",
        "  \"stream\": false,\n",
        "  \"messages\": [\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"Odpowiedz krótko na pytanie\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Kim jest Adam Mickiewicz?\"\n",
        "    }\n",
        "  ]\n",
        "}'\n",
        "```\n",
        "\n",
        "Zapytanie ma body w formie JSON, gdzie istotne są 3 parametry: `model`, `stream` i `messages`.\n",
        "W odpowiedzi otrzymany następująxcy obiekt:\n",
        "```\n",
        "{\n",
        "   \"model\":\"SpeakLeash/bielik-11b-v2.2-instruct:Q4_K_M\",\n",
        "   \"created_at\":\"2024-08-21T15:45:22.580717683Z\",\n",
        "   \"message\":{\n",
        "      \"role\":\"assistant\",\n",
        "      \"content\":\"\\u003cs\\u003e  Adam Mickiewicz to polski poeta, uważany za jednego z najważniejszych twórców literatury romantyzmu. Znany m.in. z dzieł \\\"Pan Tadeusz\\\" i \\\"Dziady\\\".\"\n",
        "   },\n",
        "   \"done_reason\":\"stop\",\n",
        "   \"done\":true,\n",
        "   \"total_duration\":967596312,\n",
        "   \"load_duration\":4041118,\n",
        "   \"prompt_eval_count\":44,\n",
        "   \"prompt_eval_duration\":34273000,\n",
        "   \"eval_count\":65,\n",
        "   \"eval_duration\":798981000\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5a2bc26b1644f82",
      "metadata": {
        "id": "a5a2bc26b1644f82"
      },
      "source": [
        "### Ollama – korzystanie z API (python)\n",
        "\n",
        "Analogiczny przykład z powyższego zapytania można wykonać w środowisku python.\n",
        "W tym celu należy zainstalować wymagane zależności oraz napisać kod jaki będzie wykonywał zapytanie.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Korzystanie z Ollama w środowisku Google Colab**\n",
        "\n",
        "Przedstawione poniżej kroki dotyczą uruchomienia Ollamy oraz pobrania modelu w środowisku Google Colab. Podaną sekcję można pominąć w przypadkiu lokalnej egzekucji kodu (kroki do niej zostały przedstawione w początkowej sekcji dokumentu)."
      ],
      "metadata": {
        "id": "vEONa5vRLYuo"
      },
      "id": "vEONa5vRLYuo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pobranie Ollamy:"
      ],
      "metadata": {
        "id": "9_cwmv9AL9SQ"
      },
      "id": "9_cwmv9AL9SQ"
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://ollama.ai/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDW1ES9v2BaN",
        "outputId": "c4e7b5f8-3478-4453-e9e9-697d2ea19217"
      },
      "id": "hDW1ES9v2BaN",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 13344    0 13344    0     0  45967      0 --:--:-- --:--:-- --:--:-- 45855\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uruchomienie Ollamy, żeby działa w tle:"
      ],
      "metadata": {
        "id": "24uBumxPMC5A"
      },
      "id": "24uBumxPMC5A"
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama serve > rocama.log 2>&1 &"
      ],
      "metadata": {
        "id": "3sVD1l216ER6"
      },
      "id": "3sVD1l216ER6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pobranie modelu (plik GGUF w formacie kwantyzacji Q4_K_M).\n",
        "```| tail -n 3``` zostało dodane, aby nie wyświetlać nadmiarowych informacji dotyczących pobierania modelu:"
      ],
      "metadata": {
        "id": "lEo2b6YiMP6_"
      },
      "id": "lEo2b6YiMP6_"
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull SpeakLeash/bielik-11b-v2.2-instruct:Q4_K_M | tail -n 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIeSka_Z73Gq",
        "outputId": "35092aff-43a5-447d-fc73-cb8fbeecac55"
      },
      "id": "EIeSka_Z73Gq",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 32354c00869f... 100% ▕▏ 6.7 GB                         \n",
            "pulling f7426507909a... 100% ▕▏  263 B                         \n",
            "pulling 3685c9d39c8b... 100% ▕▏  114 B                         \n",
            "pulling dda2439e812c... 100% ▕▏  414 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "success \u001b[?25h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RUtQdy6dKr1Z",
      "metadata": {
        "id": "RUtQdy6dKr1Z"
      },
      "source": [
        "Do celów testowych zastosowano nazwę domeny 'localhost'. Można również użyć adresu IP wskazujący na serwer Ollamy."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Poniżej znajdują się komendy instalujące biblioteki `httpx` i `requests`. Są one alternatywne i dają takie same funkcjonalności -- jeśli nie znasz żadnej wybierz `httpx`."
      ],
      "metadata": {
        "id": "tms2oEibLRpS"
      },
      "id": "tms2oEibLRpS"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "97cbdb10e1baf998",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-21T18:30:14.753392Z",
          "start_time": "2024-08-21T18:30:13.801374Z"
        },
        "id": "97cbdb10e1baf998"
      },
      "outputs": [],
      "source": [
        "!pip install -qq httpx requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "2823dd0e26750b51",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-22T15:50:18.718949Z",
          "start_time": "2024-08-22T15:50:18.714712Z"
        },
        "id": "2823dd0e26750b51"
      },
      "outputs": [],
      "source": [
        "# pprint jest dodatkowe i można użyć zwykłego print, który nie formatuje składni\n",
        "from pprint import pprint\n",
        "\n",
        "# Stałe dla zapytania — będą tak samo wykorzystane w requests i httpx\n",
        "URL = \"http://localhost:11434/api/chat\"\n",
        "PAYLOAD = {\n",
        "    \"model\": \"SpeakLeash/bielik-11b-v2.2-instruct:Q4_K_M\",\n",
        "    \"stream\": False,\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Odpowiedz krótko na pytanie\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Kim jest Adam Mickiewicz?\"\n",
        "        }\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "f3274c4c69c568c6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-22T15:50:21.497521Z",
          "start_time": "2024-08-22T15:50:18.722961Z"
        },
        "id": "f3274c4c69c568c6",
        "outputId": "c9050901-0392-47ac-cef9-c338f0217fa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'created_at': '2024-08-29T22:46:46.311720254Z',\n",
            " 'done': True,\n",
            " 'done_reason': 'stop',\n",
            " 'eval_count': 178,\n",
            " 'eval_duration': 246262319000,\n",
            " 'load_duration': 51997919094,\n",
            " 'message': {'content': 'Adam Mickiewicz był polskim poetą, dramaturgiem i '\n",
            "                        'publicystą, uznawanym za jednego z najważniejszych '\n",
            "                        'twórców romantyzmu w Polsce. Urodził się 24 grudnia '\n",
            "                        '1798 roku w Zaosiu koło Nowogródka na Litwie i zmarł '\n",
            "                        '26 listopada 1855 roku w Konstantynopolu (obecnie '\n",
            "                        'Stambuł). Jest autorem takich dzieł jak \"Pan '\n",
            "                        'Tadeusz\", \"Dziady\" czy \"Sonety krymskie\". Jego '\n",
            "                        'twórczość miała ogromny wpływ na polską literaturę i '\n",
            "                        'tożsamość narodową.',\n",
            "             'role': 'assistant'},\n",
            " 'model': 'SpeakLeash/bielik-11b-v2.2-instruct:Q4_K_M',\n",
            " 'prompt_eval_count': 103,\n",
            " 'prompt_eval_duration': 117823341000,\n",
            " 'total_duration': 416210097878}\n"
          ]
        }
      ],
      "source": [
        "import httpx\n",
        "\n",
        "response = httpx.post(URL, json=PAYLOAD, timeout=500)\n",
        "pprint(response.json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "4854db48805c9d42",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-22T15:50:22.210998Z",
          "start_time": "2024-08-22T15:50:21.498729Z"
        },
        "id": "4854db48805c9d42",
        "outputId": "4d449f20-5f25-4ff0-cdfe-94563a53f00e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'created_at': '2024-08-29T22:51:06.824372633Z',\n",
            " 'done': True,\n",
            " 'done_reason': 'stop',\n",
            " 'eval_count': 182,\n",
            " 'eval_duration': 259194295000,\n",
            " 'load_duration': 12672613,\n",
            " 'message': {'content': 'Adam Mickiewicz był polskim poetą, dramaturgiem i '\n",
            "                        'publicystą, uważanym za jednego z najwybitniejszych '\n",
            "                        'twórców romantyzmu w literaturze polskiej. Urodził '\n",
            "                        'się 24 grudnia 1798 roku w Zaosiu koło Nowogródka na '\n",
            "                        'Litwie, a zmarł 26 listopada 1855 roku w '\n",
            "                        'Konstantynopolu (obecnie Stambuł). Jest autorem '\n",
            "                        'takich dzieł jak \"Pan Tadeusz\", \"Dziady\" czy \"Sonety '\n",
            "                        'krymskie\". Jego twórczość miała ogromny wpływ na '\n",
            "                        'rozwój polskiej literatury i tożsamości narodowej.',\n",
            "             'role': 'assistant'},\n",
            " 'model': 'SpeakLeash/bielik-11b-v2.2-instruct:Q4_K_M',\n",
            " 'prompt_eval_count': 103,\n",
            " 'prompt_eval_duration': 1143376000,\n",
            " 'total_duration': 260476232123}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "response = requests.post(URL, json=PAYLOAD)\n",
        "pprint(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd0e10349026b861",
      "metadata": {
        "id": "cd0e10349026b861"
      },
      "source": [
        "## Podsumowanie\n",
        "\n",
        "W tym tutorialu przedstawiono, jak w prosty sposób uruchomić LLM na swoim komputerze. Teraz możesz całkowicie za darmo uruchomić prywatnie LLM i dowolnie z niego korzystać. W przypadku trudniejszych zadań odsyłamy do dokumentacji, gdzie opisane są bardziej zaawansowane funkcjonalności, które z pewnością pokryją bardziej złożone przypadki."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}