{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Bielik + Ollama – czyli proste uruchomienie LLM",
   "id": "b315ecd49bdab447"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "LLM można używać bezpośrednio korzystając z biblioteki **transformers** od HuggigFace, lecz na dłuższą metę może okazać się to uciążliwe – np. trzymanie modelu w obiektach i zarządzanie nimi (często powiązane z obsługą GPU).\n",
    " \n",
    "Z pomocą przychodzą różne narzędzia takie jak: **Ollama**, **vLLM**, **Text Generation Interface (TGI)** czy **llama.cpp** – w tym tutorialu ze względu na prostotę zostanie omówione pierwsze rozwiązanie."
   ],
   "id": "7eae5e1055075fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ollama\n",
    "\n",
    "Ollama to narzędzie do uruchamiania i zarządzania dużymi modelami językowymi na lokalnym komputerze. Zapewnia prosty interfejs do korzystania z gotowych modeli, jak również możliwość ich personalizacji i dostosowywania za pomocą specjalnych plików konfiguracyjnych. Ollama jest dostępna na różnych platformach, w tym macOS, Windows oraz Linux.\n",
    "\n",
    "Przydatne linki:\n",
    "  - Oficjalna strona projektu: https://ollama.com\n",
    "  - Github: https://github.com/ollama/ollama\n",
    "  - Dokumentacja API: https://github.com/ollama/ollama/blob/main/docs/api.md"
   ],
   "id": "a294d7a06f0f8712"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Ollama instalacja\n",
    "\n",
    "Narzędzie jest wspierana na platformach macOS, Windows oraz Linux. Należy zainstalować ją jako klasyczną aplikację. Na platformach Windows i macOS będzie to możliwe poprzez klasyczne pobranie z zainstalowanie pakietu, natomiast dla Linux będzie trzeba odpalić specjalną komendę *[stan na sierpień 2024]*.\n",
    "\n",
    "Szczegółowe informacje znajdują się na stronie projektu.\n",
    "\n",
    "Aplikacja działa w oparciu o Command Line Interface (CLI) – wiersz poleceń. W celu weryfikacji poprawnej instalacji zalecam uruchomienie komendy: `ollama`. Jako wynik powinno pokazać się wynik przedstawiony poniżej (dokładnie instrukcja tego narzędzia). Jeśli pojawią się błędy to znaczy że narzędzie nie zostało zainstalowane poprawnie.\n",
    "\n",
    "```shell\n",
    "(base) sample_user@MacBook-Pro SPEAKLEASH % ollama\n",
    "Large language model runner\n",
    "\n",
    "Usage:\n",
    "  ollama [flags]\n",
    "  ollama [command]\n",
    "\n",
    "Available Commands:\n",
    "  serve       Start ollama\n",
    "  create      Create a model from a Modelfile\n",
    "  show        Show information for a model\n",
    "  run         Run a model\n",
    "  pull        Pull a model from a registry\n",
    "  push        Push a model to a registry\n",
    "  list        List models\n",
    "  ps          List running models\n",
    "  cp          Copy a model\n",
    "  rm          Remove a model\n",
    "  help        Help about any command\n",
    "\n",
    "Flags:\n",
    "  -h, --help      help for ollama\n",
    "  -v, --version   Show version information\n",
    "\n",
    "Use \"ollama [command] --help\" for more information about a command.\n",
    "```"
   ],
   "id": "31033784729db073"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Ollama uruchomienie poprzez CLI\n",
    "\n",
    "Aby uruchomić LLM należy wpisać komendę:\n",
    "\n",
    "```shell\n",
    "ollama run mwiewior/bielik\n",
    "```\n",
    "\n",
    "Za pierwszym razem model będzie się pobierać na komputer co może potrwać dłuższą chwilę.\n",
    "Po poprawnym załadowaniu będzie można zacząć korzystać z modelu w konsoli.\n",
    "\n",
    "```shell\n",
    "(base) marcinwatroba@DataScienceServer:~$ ollama run mwiewior/bielik\n",
    ">>> Ile to jest 2+4?\n",
    "<s>  2 plus 4 równa się 6.\n",
    "\n",
    ">>> Kto napisał Pana Tadeusza? \n",
    "<s>  Adam Mickiewicz napisał \"Pana Tadeusza\".\n",
    "\n",
    ">>> \n",
    "```"
   ],
   "id": "9fe1a72a6ffada73"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Ollama – korzystanie z API (curl)\n",
    "\n",
    "Ollama poza korzystaniem z CLI udostępnia również możliwość komunikacji poprzez Rest API.\n",
    "Pozwala to na korzystanie z modelu w praktycznie każdej aplikacji, co daje całkiem spore możliwości.\n",
    "\n",
    "Domyślnie aplikacja działa na porcie 11434. Oznacza to że jej domyślny host to http://localhost:11434 i z niego będziemy korzystać w poniższych przykładach.\n",
    "\n",
    "Najprostszym przykładem będzie po prostu uruchomienie klasycznego zapytania dla zapytania składającego się z listy wiadomości (szczegóły dotyczące API znajdują się pod adresem https://github.com/ollama/ollama/blob/main/docs/api.md).\n",
    "\n",
    "Poniżej znajduje się polecenie zapytanie w popularnej bibliotece curl -- można jej używać w formie CLI (bez większych umiejętności programowania)"
   ],
   "id": "cc59aec5b7383d70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```shell\n",
    "curl http://localhost:11434/api/chat -d '{\n",
    "  \"model\": \"mwiewior/bielik\",\n",
    "  \"stream\": false,\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"Odpowiedz krótko na pytanie\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Kim jest Adam Mickiewicz?\"\n",
    "    }\n",
    "  ]\n",
    "}'\n",
    "```\n",
    "\n",
    "Zapytanie ma body w formie JSON, gdzie istotne są 3 parametry: `model`, `stream` i `messages`.\n",
    "W odpowiedzi otrzymany następująxcy obiekt:\n",
    "```\n",
    "{\n",
    "   \"model\":\"mwiewior/bielik\",\n",
    "   \"created_at\":\"2024-08-21T15:45:22.580717683Z\",\n",
    "   \"message\":{\n",
    "      \"role\":\"assistant\",\n",
    "      \"content\":\"\\u003cs\\u003e  Adam Mickiewicz to polski poeta, uważany za jednego z najważniejszych twórców literatury romantyzmu. Znany m.in. z dzieł \\\"Pan Tadeusz\\\" i \\\"Dziady\\\".\"\n",
    "   },\n",
    "   \"done_reason\":\"stop\",\n",
    "   \"done\":true,\n",
    "   \"total_duration\":967596312,\n",
    "   \"load_duration\":4041118,\n",
    "   \"prompt_eval_count\":44,\n",
    "   \"prompt_eval_duration\":34273000,\n",
    "   \"eval_count\":65,\n",
    "   \"eval_duration\":798981000\n",
    "}\n",
    "```"
   ],
   "id": "83761122309b676a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Ollama – korzystanie z API (python)\n",
    "\n",
    "Analogiczny przykład z powyższego zapytania można wykonać w środowisku python.\n",
    "W tym celu należy zainstalować wymagane zależności oraz napisać kod jaki będzie wykonywał zapytanie.\n",
    "\n",
    "Poniżej znajduje się skrypt instalujący biblioteki `httpx` i `requests`. Są one alternatywne i dają takie same funkcjonalności -- jeśli nie znasz żadnej wybierz `httpx`."
   ],
   "id": "a5a2bc26b1644f82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T18:30:14.753392Z",
     "start_time": "2024-08-21T18:30:13.801374Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -qq httpx requests",
   "id": "97cbdb10e1baf998",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Jeśli ten skrypt jest uruchamiany na środowisku Google Colab musimy teraz szybko skonfigurować Ollama",
   "id": "7715368a8f9f1247"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules"
   ],
   "id": "95c584fc98315248"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T18:35:04.326732Z",
     "start_time": "2024-08-21T18:35:04.323470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pprint jest dodatkowe i można użyć zwykłego print, który nie formatuje składni\n",
    "from pprint import pprint\n",
    "\n",
    "# Stałe dla zapytania — będą tak samo wykorzystane w requests i httpx\n",
    "URL = \"http://192.168.1.124:11434/api/chat\"\n",
    "PAYLOAD = {\n",
    "    \"model\": \"mwiewior/bielik\",\n",
    "    \"stream\": False,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Odpowiedz krótko na pytanie\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Kim jest Adam Mickiewicz?\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ],
   "id": "2823dd0e26750b51",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T18:34:10.370458Z",
     "start_time": "2024-08-21T18:34:09.139945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import httpx\n",
    "\n",
    "response = httpx.post(URL, json=PAYLOAD)\n",
    "pprint(response.json())"
   ],
   "id": "f3274c4c69c568c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': '2024-08-21T18:34:10.498307186Z',\n",
      " 'done': True,\n",
      " 'done_reason': 'stop',\n",
      " 'eval_count': 83,\n",
      " 'eval_duration': 1024822000,\n",
      " 'load_duration': 4644342,\n",
      " 'message': {'content': '<s>  Adam Mickiewicz to polski poeta, publicysta i '\n",
      "                        'działacz polityczny, uważany za jednego z '\n",
      "                        'najważniejszych twórców literatury polskiej. Znany '\n",
      "                        'm.in. jako autor \"Pana Tadeusza\" i zbioru ballad '\n",
      "                        '\"Ballady i romanse\".',\n",
      "             'role': 'assistant'},\n",
      " 'model': 'mwiewior/bielik',\n",
      " 'prompt_eval_count': 44,\n",
      " 'prompt_eval_duration': 34063000,\n",
      " 'total_duration': 1193780876}\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T18:35:28.920894Z",
     "start_time": "2024-08-21T18:35:27.280488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(URL, json=PAYLOAD)\n",
    "pprint(response.json())"
   ],
   "id": "4854db48805c9d42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': '2024-08-21T18:35:29.049898453Z',\n",
      " 'done': True,\n",
      " 'done_reason': 'stop',\n",
      " 'eval_count': 117,\n",
      " 'eval_duration': 1450390000,\n",
      " 'load_duration': 8581791,\n",
      " 'message': {'content': '<s>  Adam Mickiewicz jest polskim poetą i publicystą, '\n",
      "                        'uważanym za jednego z Wieszczów Narodowych. Jego '\n",
      "                        'dzieła miały duży wpływ na kulturę polską i litewską, '\n",
      "                        'propagując idee romantyzmu. Znany jest przede '\n",
      "                        'wszystkim ze swoich utworów takich jak \"Oda do '\n",
      "                        'młodości\", \"Sonety krymskie\" i \"Pan Tadeusz\".',\n",
      "             'role': 'assistant'},\n",
      " 'model': 'mwiewior/bielik',\n",
      " 'prompt_eval_count': 44,\n",
      " 'prompt_eval_duration': 32638000,\n",
      " 'total_duration': 1623777912}\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Podsumowanie\n",
    "\n",
    "Świetnie, w tym tutorialu pokazano jak w podstawowy sposób uruchomić LLM na swoim komputerze. Teraz możesz całkowicie za darmo uruchomić prywatnie LLM i dowolnie z niego korzystać. W przypadku trudniejszych zadań odsyłamy do dokumentacji gdzie będą pokazane bardziej zaawansowane możliwości które z pewnością spełnią bardziej zaawansowane wymagania."
   ],
   "id": "cd0e10349026b861"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "44a307df6032b58d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
