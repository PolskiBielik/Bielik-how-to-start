{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e32a512",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://huggingface.co/speakleash/Bielik-7B-Instruct-v0.1/raw/main/speakleash_cyfronet.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b315ecd49bdab447",
   "metadata": {
    "id": "b315ecd49bdab447"
   },
   "source": [
    "# Bielik + Ollama – czyli proste uruchomienie LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eae5e1055075fd",
   "metadata": {
    "id": "7eae5e1055075fd"
   },
   "source": [
    "LLM można używać bezpośrednio korzystając z biblioteki **transformers** od HuggigFace, lecz na dłuższą metę może okazać się to uciążliwe – np. trzymanie modelu w obiektach i zarządzanie nimi (często powiązane z obsługą GPU).\n",
    "\n",
    "Z pomocą przychodzą różne narzędzia takie jak: **Ollama**, **vLLM**, **Text Generation Interface (TGI)** czy **llama.cpp** – w tym tutorialu, ze względu na prostotę, zostanie omówione pierwsze rozwiązanie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a294d7a06f0f8712",
   "metadata": {
    "id": "a294d7a06f0f8712"
   },
   "source": [
    "## Ollama\n",
    "\n",
    "Ollama to narzędzie do uruchamiania i zarządzania dużymi modelami językowymi na lokalnym komputerze. Zapewnia prosty interfejs do korzystania z gotowych modeli, jak również możliwość ich personalizacji i dostosowywania za pomocą specjalnych plików konfiguracyjnych. Ollama jest dostępna na różnych platformach, w tym macOS, Windows oraz Linux.\n",
    "\n",
    "Przydatne linki:\n",
    "  - Oficjalna strona projektu: https://ollama.com\n",
    "  - Github: https://github.com/ollama/ollama\n",
    "  - Dokumentacja API: https://github.com/ollama/ollama/blob/main/docs/api.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31033784729db073",
   "metadata": {
    "id": "31033784729db073"
   },
   "source": [
    "### Ollama instalacja\n",
    "\n",
    "Narzędzie jest wspierane na platformach macOS, Windows oraz Linux. Należy zainstalować je jako klasyczną aplikację. Na platformach Windows i macOS będzie to możliwe poprzez klasyczne pobranie i zainstalowanie pakietu, natomiast na Linuxie będzie trzeba uruchomić specjalną komendę *[stan na sierpień 2024]*.\n",
    "\n",
    "Szczegółowe informacje znajdują się na stronie projektu.\n",
    "\n",
    "Aplikacja działa w oparciu o Command Line Interface (CLI) – wiersz poleceń. W celu weryfikacji poprawnej instalacji zaleca się  uruchomienie komendy: `ollama`. Przykładowy wynik jej działania został przedstawiony poniżej (a dokładnie instrukcja zawierająca komendy do obsługi tego narzędzia). Jeżeli pojawią się błędy, może oznaczać to, że narzędzie nie zostało zainstalowane poprawnie.\n",
    "\n",
    "```shell\n",
    "(base) sample_user@MacBook-Pro SPEAKLEASH % ollama\n",
    "Large language model runner\n",
    "\n",
    "Usage:\n",
    "  ollama [flags]\n",
    "  ollama [command]\n",
    "\n",
    "Available Commands:\n",
    "  serve       Start ollama\n",
    "  create      Create a model from a Modelfile\n",
    "  show        Show information for a model\n",
    "  run         Run a model\n",
    "  pull        Pull a model from a registry\n",
    "  push        Push a model to a registry\n",
    "  list        List models\n",
    "  ps          List running models\n",
    "  cp          Copy a model\n",
    "  rm          Remove a model\n",
    "  help        Help about any command\n",
    "\n",
    "Flags:\n",
    "  -h, --help      help for ollama\n",
    "  -v, --version   Show version information\n",
    "\n",
    "Use \"ollama [command] --help\" for more information about a command.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe1a72a6ffada73",
   "metadata": {
    "id": "9fe1a72a6ffada73"
   },
   "source": [
    "### Ollama uruchomienie poprzez CLI\n",
    "\n",
    "Aby uruchomić LLM należy wpisać komendę:\n",
    "\n",
    "```shell\n",
    "ollama run SpeakLeash/bielik-7b-instruct-v0.1-gguf\n",
    "```\n",
    "\n",
    "Za pierwszym razem model będzie się pobierał na nasz komputer, co może potrwać dłuższą chwilę.\n",
    "Po poprawnym załadowaniu będzie można zacząć korzystać z modelu w konsoli.\n",
    "\n",
    "```shell\n",
    "(base) marcinwatroba@DataScienceServer:~$ ollama run SpeakLeash/bielik-7b-instruct-v0.1-gguf\n",
    ">>> Ile to jest 2+4?\n",
    "<s>  2 plus 4 równa się 6.\n",
    "\n",
    ">>> Kto napisał Pana Tadeusza?\n",
    "<s>  Adam Mickiewicz napisał \"Pana Tadeusza\".\n",
    "\n",
    ">>>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc59aec5b7383d70",
   "metadata": {
    "id": "cc59aec5b7383d70"
   },
   "source": [
    "### Ollama – korzystanie z API (curl)\n",
    "\n",
    "Ollama, poza korzystaniem z CLI, udostępnia również możliwość komunikacji poprzez Rest API.\n",
    "Pozwala to na korzystanie z modelu w praktycznie każdej aplikacji, co daje całkiem spore możliwości.\n",
    "\n",
    "Domyślnie aplikacja działa na porcie 11434. Oznacza to, że jej domyślny host to http://localhost:11434 i z niego będziemy korzystać w poniższych przykładach.\n",
    "\n",
    "Najprostszym przykładem będzie po prostu uruchomienie klasycznego zapytania dla zapytania składającego się z listy wiadomości (szczegóły dotyczące API znajdują się pod adresem https://github.com/ollama/ollama/blob/main/docs/api.md).\n",
    "\n",
    "Poniżej znajduje się przykład zapytania w popularnej bibliotece `curl` -- można jej używać w formie CLI, bez większych umiejętności programowania."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83761122309b676a",
   "metadata": {
    "id": "83761122309b676a"
   },
   "source": [
    "```shell\n",
    "curl http://localhost:11434/api/chat -d '{\n",
    "  \"model\": \"SpeakLeash/bielik-7b-instruct-v0.1-gguf\",\n",
    "  \"stream\": false,\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"Odpowiedz krótko na pytanie\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Kim jest Adam Mickiewicz?\"\n",
    "    }\n",
    "  ]\n",
    "}'\n",
    "```\n",
    "\n",
    "Zapytanie ma body w formie JSON, gdzie istotne są 3 parametry: `model`, `stream` i `messages`.\n",
    "W odpowiedzi otrzymany następująxcy obiekt:\n",
    "```\n",
    "{\n",
    "   \"model\":\"SpeakLeash/bielik-7b-instruct-v0.1-gguf\",\n",
    "   \"created_at\":\"2024-08-21T15:45:22.580717683Z\",\n",
    "   \"message\":{\n",
    "      \"role\":\"assistant\",\n",
    "      \"content\":\"\\u003cs\\u003e  Adam Mickiewicz to polski poeta, uważany za jednego z najważniejszych twórców literatury romantyzmu. Znany m.in. z dzieł \\\"Pan Tadeusz\\\" i \\\"Dziady\\\".\"\n",
    "   },\n",
    "   \"done_reason\":\"stop\",\n",
    "   \"done\":true,\n",
    "   \"total_duration\":967596312,\n",
    "   \"load_duration\":4041118,\n",
    "   \"prompt_eval_count\":44,\n",
    "   \"prompt_eval_duration\":34273000,\n",
    "   \"eval_count\":65,\n",
    "   \"eval_duration\":798981000\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2bc26b1644f82",
   "metadata": {
    "id": "a5a2bc26b1644f82"
   },
   "source": [
    "### Ollama – korzystanie z API (python)\n",
    "\n",
    "Analogiczny przykład z powyższego zapytania można wykonać w środowisku python.\n",
    "W tym celu należy zainstalować wymagane zależności oraz napisać kod jaki będzie wykonywał zapytanie.\n",
    "\n",
    "Poniżej znajduje się skrypt instalujący biblioteki `httpx` i `requests`. Są one alternatywne i dają takie same funkcjonalności -- jeśli nie znasz żadnej wybierz `httpx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cbdb10e1baf998",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T18:30:14.753392Z",
     "start_time": "2024-08-21T18:30:13.801374Z"
    },
    "id": "97cbdb10e1baf998"
   },
   "outputs": [],
   "source": [
    "!pip install -qq httpx requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RUtQdy6dKr1Z",
   "metadata": {
    "id": "RUtQdy6dKr1Z"
   },
   "source": [
    "Do celów testowych zastosowano nazwę domeny 'localhost'. Można również użyć adresu IP wskazujący na serwer Ollamy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2823dd0e26750b51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T15:50:18.718949Z",
     "start_time": "2024-08-22T15:50:18.714712Z"
    },
    "id": "2823dd0e26750b51"
   },
   "outputs": [],
   "source": [
    "# pprint jest dodatkowe i można użyć zwykłego print, który nie formatuje składni\n",
    "from pprint import pprint\n",
    "\n",
    "# Stałe dla zapytania — będą tak samo wykorzystane w requests i httpx\n",
    "URL = \"http://localhost:11434/api/chat\"\n",
    "PAYLOAD = {\n",
    "    \"model\": \"SpeakLeash/bielik-7b-instruct-v0.1-gguf\",\n",
    "    \"stream\": False,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Odpowiedz krótko na pytanie\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Kim jest Adam Mickiewicz?\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3274c4c69c568c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T15:50:21.497521Z",
     "start_time": "2024-08-22T15:50:18.722961Z"
    },
    "id": "f3274c4c69c568c6",
    "outputId": "ab93f864-076f-4311-abb8-10765f9eedb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': '2024-08-22T15:50:21.438020749Z',\n",
      " 'done': True,\n",
      " 'done_reason': 'stop',\n",
      " 'eval_count': 75,\n",
      " 'eval_duration': 646169000,\n",
      " 'load_duration': 1891913891,\n",
      " 'message': {'content': ' Adam Mickiewicz to polski poeta, który brał udział w '\n",
      "                        'powstaniu listopadowym i uważany jest za jednego z '\n",
      "                        'Wieszczów Narodowych. Jego dzieła mocno oddziaływały '\n",
      "                        'na polską kulturę i tożsamość narodową.',\n",
      "             'role': 'assistant'},\n",
      " 'model': 'SpeakLeash/bielik-7b-instruct-v0.1-gguf',\n",
      " 'prompt_eval_count': 41,\n",
      " 'prompt_eval_duration': 20510000,\n",
      " 'total_duration': 2652471217}\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "\n",
    "response = httpx.post(URL, json=PAYLOAD, timeout=500)\n",
    "pprint(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4854db48805c9d42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T15:50:22.210998Z",
     "start_time": "2024-08-22T15:50:21.498729Z"
    },
    "id": "4854db48805c9d42",
    "outputId": "c5ea3b29-7077-4fc3-a2ab-549240cf1bdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': '2024-08-22T15:50:22.192379581Z',\n",
      " 'done': True,\n",
      " 'done_reason': 'stop',\n",
      " 'eval_count': 57,\n",
      " 'eval_duration': 488890000,\n",
      " 'load_duration': 4256853,\n",
      " 'message': {'content': ' Adam Mickiewicz to polski poeta, publicysta i '\n",
      "                        'filozof, uważany za jednego z Wieszczów Narodowych. '\n",
      "                        'Jest znany przede wszystkim jako autor epopei \"Pan '\n",
      "                        'Tadeusz\".',\n",
      "             'role': 'assistant'},\n",
      " 'model': 'SpeakLeash/bielik-7b-instruct-v0.1-gguf',\n",
      " 'prompt_eval_count': 41,\n",
      " 'prompt_eval_duration': 8792000,\n",
      " 'total_duration': 646232811}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(URL, json=PAYLOAD)\n",
    "pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e10349026b861",
   "metadata": {
    "id": "cd0e10349026b861"
   },
   "source": [
    "## Podsumowanie\n",
    "\n",
    "W tym tutorialu przedstawiono, jak w prosty sposób uruchomić LLM na swoim komputerze. Teraz możesz całkowicie za darmo uruchomić prywatnie LLM i dowolnie z niego korzystać. W przypadku trudniejszych zadań odsyłamy do dokumentacji, gdzie opisane są bardziej zaawansowane funkcjonalności, które z pewnością pokryją bardziej złożone przypadki."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
