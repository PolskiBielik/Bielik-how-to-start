{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V55IAMNhYrP9"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://huggingface.co/speakleash/Bielik-7B-Instruct-v0.1/raw/main/speakleash_cyfronet.png\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLjD5NSinlFS"
      },
      "source": [
        "W tym notebooku przejdziemy całą ścieżkę - od uruchomienia modelu aż po zwrot danych w formacie JSON. Zaczynajmy! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpi-lp87jeIu"
      },
      "source": [
        "### Przygotowanie środowiska"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "07-4DrcSdEf8"
      },
      "outputs": [],
      "source": [
        "#%%capture\n",
        "\n",
        "!pip install accelerate -q\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2iD0O366dg0K"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YfVY6E7ge6bT"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") # tylko na potrzeby naszego demo ;)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gated models\n",
        "\n",
        "W przypadku modeli, na które zostały nałożone zabezpieczenia w postaci kontrolowanego dostępu (gated models), wymagane będzie podanie tokena użytkownika, dostępnego na portalu HuggingFace:\n",
        "\n",
        "* informacje dotyczące tokenów:\n",
        "https://huggingface.co/docs/hub/security-tokens\n",
        "* token użytkownika\n",
        "https://huggingface.co/settings/tokens\n",
        "\n",
        "W tym celu należy zainstalować bibliotekę `huggingface_hub[cli]` oraz dokonać logowania za pomocą tokena, co zostało zaprezentowane w poniższych 2 komórkach."
      ],
      "metadata": {
        "id": "3By23rs7ITut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"huggingface_hub[cli]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__htdtttIY4x",
        "outputId": "0418ceed-0d9b-4890-a9db-d61300ac7951"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub[cli] in /usr/local/lib/python3.10/dist-packages (0.24.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (4.12.2)\n",
            "Requirement already satisfied: InquirerPy==0.3.4 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (0.3.4)\n",
            "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (0.3.4)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.47)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (2024.7.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2W_dhzzIaZh",
        "outputId": "de5cfde2-31f7-457e-dcb6-7d20eb192539"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: fineGrained).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqfY6a1qgvYi"
      },
      "source": [
        "### Inicjalizacja modelu (4 bit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "72d384bd2b054ccca27bff69e110f479",
            "5203beef1f274bd7b18081bdf8fbbae6",
            "f7e326535c2e4e6bb8fb0b3decc0efae",
            "77995d268e324180969403b6e53a52b7",
            "1c46cfcd61b84565b8917a11c7c2fff2",
            "07ab0540b12a4143805555fd171af2af",
            "36ed31d24d144bd2b07fe572e8121fcb",
            "59cd1a1376494875a10492ade42ed11f",
            "d00bd8d3c8f44b3ba7ab707884920a2b",
            "deeffa8f3fa04987b37486475fd59a3a",
            "e299784e399a46f99272db64a93b50bb"
          ]
        },
        "id": "vfWiekAIt0DR",
        "outputId": "c93e874d-b9ea-48b5-90f5-df0cc889155b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72d384bd2b054ccca27bff69e110f479"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextStreamer\n",
        "\n",
        "device = \"cuda\"\n",
        "model_name = \"speakleash/Bielik-11B-v2.2-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             torch_dtype=torch.bfloat16,\n",
        "                                             quantization_config=quantization_config\n",
        "                                             )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMT64Pa06HR1"
      },
      "source": [
        "### Funkcja pomocnicza do generowania odpowiedzi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8fyc3sKm4n3J"
      },
      "outputs": [],
      "source": [
        "def generate(prompt: str,\n",
        "             max_tokens: int = 256,\n",
        "             temperature: float = 0.0,\n",
        "             top_k: int = 100,\n",
        "             top_p: int = 1,\n",
        "             system: str = None,\n",
        "             return_str: bool = False):\n",
        "\n",
        "  messages = []\n",
        "\n",
        "  # Przygotowanie template z promptem\n",
        "  if system:\n",
        "    messages.append({\"role\": \"system\",\n",
        "                     \"content\": system})\n",
        "\n",
        "  messages.append({\"role\": \"user\",\n",
        "                   \"content\": prompt})\n",
        "\n",
        "  input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "  model_inputs = input_ids\n",
        "\n",
        "  # Przerzucenie inputu na GPU, jeśli jest dostępne\n",
        "  if torch.cuda.is_available():\n",
        "    model_inputs = input_ids.to(device)\n",
        "\n",
        "  # Generowanie odpowiedzi przez model\n",
        "  outputs = model.generate(\n",
        "      model_inputs,\n",
        "      streamer=streamer,\n",
        "      max_new_tokens=max_tokens,\n",
        "      do_sample=True if temperature else False,\n",
        "      temperature=temperature,\n",
        "      top_p=top_p,\n",
        "      top_k=top_k)\n",
        "\n",
        "  # Wydrukowanie odpowiedzi modelu bądź zwrotka do zmiennej\n",
        "  if return_str:\n",
        "    decoded = tokenizer.batch_decode(outputs)\n",
        "    return decoded[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnY_3s0u4My5"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test z promptem systemowym i poleceniem od użytkownika\n",
        "temperature = 0.1\n",
        "max_tokens = 1000\n",
        "\n",
        "generate(prompt=\"Hej, powiedz mi czym się zajmujesz!\",\n",
        "         system=\"Jesteś programistą baz danych, który chce rozwijać swoje kompetencje w obszarze AI\",\n",
        "         temperature=temperature,\n",
        "         max_tokens=max_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIBBYschNMYl",
        "outputId": "7d5d41a0-6c72-4750-cb2e-0449ec95293a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:32001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Jestem programistą baz danych, ale chciałbym rozwijać swoje umiejętności w zakresie sztucznej inteligencji (AI). Obecnie pracuję głównie z bazami danych SQL, takimi jak PostgreSQL i MySQL, oraz z systemami zarządzania bazami danych (DBMS), takimi jak Oracle Database.\n",
            "\n",
            "Moja praca obejmuje tworzenie i utrzymywanie schematów baz danych, optymalizację zapytań SQL, zarządzanie indeksami, oraz rozwiązywanie problemów z wydajnością. Często współpracuję z analitykami danych i programistami aplikacji, aby zapewnić płynną wymianę danych między bazą danych a aplikacjami.\n",
            "\n",
            "Teraz chciałbym rozszerzyć swoje kompetencje o AI, ponieważ widzę, że technologia ta ma ogromny potencjał w analizie i przetwarzaniu danych. Chciałbym nauczyć się, jak wykorzystywać AI do automatyzacji niektórych zadań, tworzenia modeli predykcyjnych oraz ulepszania procesów analitycznych w bazach danych.\n",
            "\n",
            "Czy możesz mi doradzić, jakie umiejętności i technologie powinienem opanować, aby skutecznie łączyć swoją wiedzę o bazach danych z AI?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Przykład 1\n",
        "Bielik świetnie nadaje się do analizy tekstu i odpowiedzi w formacie obiektu JSON. Sprawdźmy zatem jego możliwości na żywo!\n",
        "\n",
        "Na warsztat weźmy przykładowy artykuł: https://wydarzenia.interia.pl/kraj/news-ekstremalne-warunki-niemal-w-calym-kraju-sypia-sie-alerty,nId,7763701"
      ],
      "metadata": {
        "id": "uiEu7jE2NSwd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Najpierw musimy wydobyć tekst z artykułu! Do tego przydadzą się pythonowe biblioteki **requests** oraz **justext**!"
      ],
      "metadata": {
        "id": "7se5zOnw1cob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalacja niezbędnych bibliotek\n",
        "!pip install justext -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUGrw5hi1a-j",
        "outputId": "3cf63106-b3a2-42a8-802f-1a9414fec67c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: lxml 4.9.4 does not provide the extra 'html-clean'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import justext\n",
        "\n",
        "url = \"https://wydarzenia.interia.pl/kraj/news-ekstremalne-warunki-niemal-w-calym-kraju-sypia-sie-alerty,nId,7763701\"\n",
        "\n",
        "def get_article_from_url(url: str):\n",
        "  # Przygotowanie zmiennej do zbierania fragmentów artykułu ze strony internetowej\n",
        "  article_parts = []\n",
        "\n",
        "  # Zebranie odpowiedzi w HTML\n",
        "  response = requests.get(url)\n",
        "\n",
        "  # Przetworzenie odpowiedzi przy użyciu biblioteki justext\n",
        "  paragraphs = justext.justext(response.content, justext.get_stoplist(\"Polish\"))\n",
        "  for paragraph in paragraphs:\n",
        "    if not paragraph.is_boilerplate:\n",
        "      article_parts.append(paragraph.text)\n",
        "\n",
        "  return \" \".join(article_parts)"
      ],
      "metadata": {
        "id": "9EwpmHIU1cOw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test działania funkcji\n",
        "get_article_from_url(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "o64Gdwl32bsz",
        "outputId": "fe052e57-b788-46f2-bc33-e79436242616"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ekstremalne warunki niemal w całym kraju. Sypią się alerty Pogoda w najbliższym czasie nie da odetchnąć. Niedziela będzie kolejnym dniem z tropikalnymi upałami. W wielu miejscach termometry wskażą powyżej 30 stopni Celsjusza. W związku z ekstremalnymi warunkami IMGW wydał ostrzeżenia dla ponad połowy kraju. W lasach obowiązuje bardzo wysokie zagrożenie pożarowe. Fala upałów w Polsce. IMGW wydał ostrzeżenia/Zbyszek Kaczmarek/Agencja FORUM Reklama W niedzielę z nieba znów poleje się żar. Niemal wszędzie notowane będą ponad 30-stopniowe upały. W związku z ekstremalnymi warunkami IMGW wydał ostrzeżenia pierwszego i drugiego stopnia dla połowy kraju. Fala upałów w Polsce. IMGW wydał ostrzeżenia dla połowy kraju W ciągu dnia na wschodzie i południu Polski niebo będzie bezchmurne. Na pozostałym obszarze może wystąpić przelotny deszcz, a lokalnie - także burze. Wiatr, szczególnie nad morzem będzie dość silny. Porywy mogą sięgać do 65 km/h. Reklama Będzie upalnie. Niemal w całej Polsce słupki rtęci powędrują w okolicę 30 stopni Celsjusza, a w wielu miejscach przekroczą tę granicę. Najcieplej będzie w Lublinie, Rzeszowie i Warszawie - tam termometry wskażą nawet 34 kreski. IMGW wydał ostrzeżenia drugiego stopnia dla centralnej, południowej i wschodniej części kraju. Alerty obowiązują w województwach: warmińsko-mazurskim, podlaskim, mazowiecki, świętokrzyskim, łódzkim, opolskim oraz śląskim, a także dla części województwa lubelskiego podkarpackiego, małopolskiego i kujawsko-pomorskiego. Susza i zagrożenie pożarowe. \"Zachowajmy ostrożność\" Pierwszy stopień ostrzeżeń obowiązuje z kolei dla prawie całego województwa małopolskiego, lubelskiego i podkarpackiego. IMGW wydał również 36 alertów o suszy hydrologicznej, w większości dla dorzecza Wisły. \"Gorące, suche dni sprzyjają wzrostowi zagrożenia pożarowego. W związku z tym zachowajmy ostrożność planując wycieczki na łono natury\" - czytamy w komunikacie Instytutu. W lasach obowiązuje ekstremalne, bardzo wysokie oraz wysokie ryzyko pożaru. Reklama Synoptycy przewidują, że 30-stopniowe upały potrwają do końca przyszłego tygodnia. Początek września ma przynieść niewielkie ochłodzenie.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpvjcGVFpxu4",
        "outputId": "f32d0b5f-61e9-4eb3-a96d-76660b003b3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:32001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Na podstawie podanego tekstu, oto propozycja struktury JSON z kluczami, które uważam za istotne:\n",
            "\n",
            "```json\n",
            "{\n",
            " \"temperatura\": {\n",
            "   \"zakres\": \"30-34°C\",\n",
            "   \"miasta\": [\"Lublin\", \"Rzeszów\", \"Warszawa\"]\n",
            " },\n",
            " \"ostrzeżenia\": {\n",
            "   \"stopnie\": [\"pierwszy\", \"drugi\"],\n",
            "   \"województwa\": [\n",
            "     \"drugiego stopnia\": [\"warmińsko-mazurskie\", \"podlaskie\", \"mazowieckie\", \"świętokrzyskie\", \"łódzkie\", \"opolskie\", \"śląskie\", \"lubelskie\", \"podkarpackie\", \"małopolskie\", \"kujawsko-pomorskie\"],\n",
            "     \"pierwszego stopnia\": [\"małopolskie\", \"lubelskie\", \"podkarpackie\"]\n",
            "   ]\n",
            " },\n",
            " \"zagrożenie_pożarowe\": {\n",
            "   \"poziom\": \"ekstremalne, bardzo wysokie oraz wysokie\",\n",
            "   \"komunikat\": \"Gorące, suche dni sprzyjają wzrostowi zagrożenia pożarowego. W związku z tym zachowajmy ostrożność planując wycieczki na łono natury.\"\n",
            " },\n",
            " \"prognoza\": {\n",
            "   \"do_kiedy\": \"do końca przyszłego tygodnia\",\n",
            "   \"początek_września\": \"niewielkie ochłodzenie\"\n",
            " },\n",
            " \"pogoda\": {\n",
            "   \"burze\": true,\n",
            "   \"wiatr\": {\n",
            "     \"nad_morzem\": \"dość silny\",\n",
            "     \"porywy\": \"do 65 km/h\"\n",
            "   }\n",
            " },\n",
            " \"susza\": {\n",
            "   \"liczba_alertów\": 36,\n",
            "   \"dorzecze\": \"Wisły\"\n",
            " }\n",
            "}\n",
            "```\n",
            "\n",
            "Ta struktura JSON zawiera kluczowe informacje z tekstu, takie jak zakres temperatur, ostrzeżenia pogodowe, zagrożenie pożarowe, prognoza długoterminowa oraz informacje o burzach i suszy. Uważam, że te dane są najbardziej istotne i użyteczne dla użytkownika końcowego.\n"
          ]
        }
      ],
      "source": [
        "# Pobranie treści artykułu do zmiennej\n",
        "article = get_article_from_url(url)\n",
        "\n",
        "# Przygotowanie promptów\n",
        "system = 'Jesteś ekspertem od ekstrakcji danych z tekstów i strukturyzowania ich do formatu obiektów JSON. Przeanalizuj otrzymany tekst i zwróć odpowiedź z formacie JSON z istotnymi według Ciebie kluczami.'\n",
        "prompt = f\"Tekst do analizy: {article}\"\n",
        "\n",
        "# Zwiększenie limitu tokenów i obniżenie temperatury (model mniej improwizuje)\n",
        "temperature = 0.0\n",
        "max_tokens = 1000\n",
        "\n",
        "# Generowanie odpowiedzi Bielika!\n",
        "generate(prompt=prompt,\n",
        "         system=system,\n",
        "         max_tokens=max_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctgLpx6NzCEQ"
      },
      "source": [
        "# Przykład 2\n",
        "\n",
        "Wykorzystamy ten sam artykuł, ale damy modelowi nieco większą swobodę twórczą w prompcie!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QI_BQbT7zO_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a875aa3-a41c-4c2e-c5a9-3a9761c3f990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:32001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Oto strukturyzowane dane w formacie JSON:\n",
            "\n",
            "```json\n",
            "{\n",
            " \"temperature\": {\n",
            "   \"max\": \"34°C\",\n",
            "   \"min\": \"30°C\"\n",
            " },\n",
            " \"regions\": [\n",
            "   \"centralna\",\n",
            "   \"południowa\",\n",
            "   \"wschodnia\"\n",
            " ],\n",
            " \"wojewodztwa\": [\n",
            "   \"warmińsko-mazurskie\",\n",
            "   \"podlaskie\",\n",
            "   \"mazowieckie\",\n",
            "   \"świętokrzyskie\",\n",
            "   \"łódzkie\",\n",
            "   \"opolskie\",\n",
            "   \"śląskie\",\n",
            "   \"lubelskie\",\n",
            "   \"podkarpackie\",\n",
            "   \"małopolskie\",\n",
            "   \"kujawsko-pomorskie\"\n",
            " ],\n",
            " \"ostrzezenia\": {\n",
            "   \"stopien1\": [\n",
            "     \"województwo małopolskie\",\n",
            "     \"województwo lubelskie\",\n",
            "     \"województwo podkarpackie\"\n",
            "   ],\n",
            "   \"stopien2\": [\n",
            "     \"centralna\",\n",
            "     \"południowa\",\n",
            "     \"wschodnia\"\n",
            "   ],\n",
            "   \"susza\": \"36 alertów o suszy hydrologicznej, w większości dla dorzecza Wisły\"\n",
            " },\n",
            " \"zagrozenie_pozarowe\": \"ekstremalne, bardzo wysokie oraz wysokie ryzyko pożaru\",\n",
            " \"czas_trwania\": \"do końca przyszłego tygodnia\",\n",
            "   \"poczatek_wrzesnia\": \"niewielkie ochłodzenie\"\n",
            "}\n",
            "```\n",
            "\n",
            "Ten format JSON zawiera kluczowe informacje z tekstu, takie jak maksymalne i minimalne temperatury, regiony i województwa objęte ostrzeżeniami, rodzaje ostrzeżeń, zagrożenie pożarowe, czas trwania upałów oraz prognozowane ochłodzenie na początku września.\n"
          ]
        }
      ],
      "source": [
        "article = get_article_from_url(url)\n",
        "\n",
        "system = 'Jesteś ekspertem od ekstrakcji danych z artykułów i strukturyzowania ich do formatu obiektów JSON. Przeanalizuj otrzymany tekst i zwróć odpowiedź z formacie JSON.'\n",
        "prompt = f'Tekst do analizy: {article}'\n",
        "max_tokens = 1000\n",
        "\n",
        "generate(prompt=prompt,\n",
        "         system=system,\n",
        "         max_tokens=max_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIE2Tl-c6YRm"
      },
      "source": [
        "# Przykład 3\n",
        "\n",
        "Dla odmiany poprosimy Bielika o zwrócenie informacji tylko dla zadanych przez nas zagadnień (bądź kluczy mówiąc językiem obiektów JSON)..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MPBNIITq_Ug8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37b61981-05bb-44dd-f6eb-b5ccfe77115c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:32001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Przeanalizowałem tekst i oto obiekt JSON zgodny z Twoim wymaganiem:\n",
            "\n",
            "```json\n",
            "{\n",
            " \"dzień_tygodnia\": \"niedziela\",\n",
            " \"data\": \"2023-08-20\",\n",
            " \"temperatura\": {\n",
            "   \"maksymalna\": \"34°C\",\n",
            "   \"lokalizacje\": [\"Lublin\", \"Rzeszów\", \"Warszawa\"]\n",
            " },\n",
            " \"zagrożenia\": {\n",
            "   \"pożarowe\": \"ekstremalne, bardzo wysokie oraz wysokie\",\n",
            "   \"susza\": \"36 alertów o suszy hydrologicznej\"\n",
            " },\n",
            " \"prognoza_na_kolejne_dni\": \"30-stopniowe upały potrwają do końca przyszłego tygodnia, początek września ma przynieść niewielkie ochłodzenie\"\n",
            "}\n",
            "```\n",
            "\n",
            "Wyjaśnienie:\n",
            "\n",
            "1. Dzień tygodnia: \"niedziela\"\n",
            "2. Data: Chociaż nie jest podana dokładna data, można założyć, że jest to aktualny dzień (w tym przypadku 20 sierpnia 2023).\n",
            "3. Temperatura:\n",
            "  - Maksymalna: 34°C\n",
            "  - Lokalizacje: Lublin, Rzeszów, Warszawa\n",
            "4. Zagrożenia:\n",
            "  - Pożarowe: Ekstremalne, bardzo wysokie oraz wysokie ryzyko pożaru\n",
            "  - Susza: 36 alertów o suszy hydrologicznej\n",
            "5. Prognoza na kolejne dni: 30-stopniowe upały potrwają do końca przyszłego tygodnia, początek września ma przynieść niewielkie ochłodzenie\n",
            "\n",
            "Ten format JSON zawiera kluczowe informacje z artykułu w strukturalnej formie, zgodnie z Twoim wymaganiem.\n"
          ]
        }
      ],
      "source": [
        "article = get_article_from_url(url)\n",
        "\n",
        "system = 'Jesteś ekspertem od ekstrakcji danych z artykułów i strukturyzowania ich do formatu obiektów JSON. Przeanalizuj otrzymany tekst i zwróć go w formacie JSON. Klucze, które powinien zawierać obiekt JSON to: dzień_tygodnia, data, temperatura, zagrożenia, prognoza_na_kolejne_dni.'\n",
        "prompt = f\"Tekst do analizy: {article}\"\n",
        "max_tokens = 1000\n",
        "\n",
        "generate(prompt=prompt,\n",
        "         system=system,\n",
        "         max_tokens=max_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Podsumowanie\n",
        "Jak widać, ekstrakcja danych do formatu JSON z tekstu jest dziecinnie prosta dla Bielika. Wystarczy krótki, prosty prompt, darmowe GPU na Colabie i voila - dane w JSONie gotowe!\n",
        "\n",
        "Zapraszam do sprawdzenia pozostałych notebooków i tutoriali przygotowanych przez naszą społeczność!"
      ],
      "metadata": {
        "id": "TiAroHoZ_4NF"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "72d384bd2b054ccca27bff69e110f479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5203beef1f274bd7b18081bdf8fbbae6",
              "IPY_MODEL_f7e326535c2e4e6bb8fb0b3decc0efae",
              "IPY_MODEL_77995d268e324180969403b6e53a52b7"
            ],
            "layout": "IPY_MODEL_1c46cfcd61b84565b8917a11c7c2fff2"
          }
        },
        "5203beef1f274bd7b18081bdf8fbbae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07ab0540b12a4143805555fd171af2af",
            "placeholder": "​",
            "style": "IPY_MODEL_36ed31d24d144bd2b07fe572e8121fcb",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f7e326535c2e4e6bb8fb0b3decc0efae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59cd1a1376494875a10492ade42ed11f",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d00bd8d3c8f44b3ba7ab707884920a2b",
            "value": 5
          }
        },
        "77995d268e324180969403b6e53a52b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_deeffa8f3fa04987b37486475fd59a3a",
            "placeholder": "​",
            "style": "IPY_MODEL_e299784e399a46f99272db64a93b50bb",
            "value": " 5/5 [01:50&lt;00:00, 20.08s/it]"
          }
        },
        "1c46cfcd61b84565b8917a11c7c2fff2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07ab0540b12a4143805555fd171af2af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36ed31d24d144bd2b07fe572e8121fcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59cd1a1376494875a10492ade42ed11f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d00bd8d3c8f44b3ba7ab707884920a2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "deeffa8f3fa04987b37486475fd59a3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e299784e399a46f99272db64a93b50bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}