{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auYFDEJyVcpI"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q streamlit accelerate transformers ctransformers[cuda]\n",
        "!npm install -g localtunnel"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sDfhIUWMXV0a"
      },
      "source": [
        "## Prosta aplikacja z wykorzystaniem Streamlit i Bielika w wersji skwantyzowanej GGUF (Q4)\n",
        "Metoda kwantyzacji: Group-wise Gradient Update Filtering (GGUF)\n",
        "Rozmiar: Q4_K_M\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcT43Zm_X_Eh",
        "outputId": "09c27023-2551-4909-c2f6-e468c3506622"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from ctransformers import AutoModelForCausalLM\n",
        "from transformers import AutoTokenizer, TextIteratorStreamer\n",
        "from threading import Thread\n",
        "\n",
        "@st.cache_resource      # Dekorator, żeby model i inne rzeczy były trzymane w pamięci cache\n",
        "def prepare_model() -> tuple:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"speakleash/Bielik-7B-Instruct-v0.1\")\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"speakleash/Bielik-7B-Instruct-v0.1-GGUF\",\n",
        "        model_file=\"bielik-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "        model_type=\"mistral\", gpu_layers=50, hf=True)\n",
        "\n",
        "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "    return tokenizer, model, streamer\n",
        "\n",
        "\n",
        "def generate_text(tokenizer, model, streamer, prompt, max_tokens = 512) -> str:\n",
        "    # Przygotowanie promptu przez tokenizer\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Przygotowanie zmiennych dla modelu\n",
        "    generation_kwargs = {\n",
        "        \"input_ids\": inputs[\"input_ids\"],\n",
        "        \"max_new_tokens\": max_tokens,\n",
        "        \"streamer\": streamer\n",
        "    }\n",
        "\n",
        "    # Uruchamiamy generowanie modelu w osobnym wątku\n",
        "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "    thread.start()\n",
        "\n",
        "    for output in streamer:\n",
        "        yield output    # Zwracamy fragmenty odpowiedzi\n",
        "\n",
        "\n",
        "### ------------------------------------------------------------------------ ###\n",
        "# Aplikacja Streamlit\n",
        "\n",
        "# Historia czatu\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "\n",
        "st.title(\"Bielik-v0.1-7B (Q4) LLM App\")\n",
        "\n",
        "# Ładowanie modelu\n",
        "with st.status(\"Pobieranie i przygotowanie modelu...\", expanded=True) as status:\n",
        "    tokenizer, model, streamer = prepare_model()\n",
        "    status.update(label=\"Model gotowy do rozmowy!\", state=\"complete\", expanded=False)\n",
        "\n",
        "# Historia konwersacji\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# Czat z modelem\n",
        "if human_prompt := st.chat_input(\"Witaj, o czym dzisiaj porozmawiamy?\"):\n",
        "    # Dodanie specjalnych tokenów, żeby model działał poprawnie\n",
        "    prompt = f\"<s>[INST]{human_prompt}[/INST]\"\n",
        "\n",
        "    # Zapisanie promptu do historii\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": human_prompt})\n",
        "\n",
        "    # Pokazanie wiadomości od użytkownika\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(human_prompt)\n",
        "\n",
        "    # Odpowiedź modelu\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        # Streaming - czyli wiadomość będzie uzupełniana jak tylko model zwróci kolejny token\n",
        "        # czyli będą się pojawiać kolejne tokeny i nie będziemy czekać na całą wiadomość\n",
        "        response_stream = st.write_stream(generate_text(tokenizer, model, streamer, prompt))\n",
        "\n",
        "    # Zapisanie odpowiedzi modelu do historii czatu\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response_stream})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hBLUhsIYnNm",
        "outputId": "c6e4d9e7-e93e-48de-ce28-8f25e93e96a4"
      },
      "outputs": [],
      "source": [
        "# Po uruchomieniu będzie adres IP - należy go skopiować i przejść na stronę, która będzie poniżej\n",
        "!streamlit run app.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
